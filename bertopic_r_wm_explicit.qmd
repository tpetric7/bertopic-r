---
title: "Topic Modeling with BERTopic in R via reticulate and groq"
subtitle: "Words & Music: Rap & Rock"
author: "Teodor Petriƒç"
date: "2024-08-26"
format:
  html:
    theme:
      light: flatly
      dark: darkly
    # css: [style.css, TMwR.css]
    # scss: [colors.scss]
    # scss: [r4ds.scss]
    # page-layout: full
    toc: true
    toc-depth: 5
    toc-location: right
    code-fold: true
    code-summary: "Show the code"
    code-tools: true
    code-copy: hover
    code-overflow: wrap
    code-link: false # set to false for plotly
    number-sections: true
    number-depth: 5
    callout-appearance: default
    callout-icon: true
    citations-hover: true
    footnotes-hover: true
    fig-width: 8
    fig-height: 6
    fig-align: left
    fig-responsive: true
    fig-cap-location: bottom
    tbl-cap-location: top
    lang: en
    self-contained: true
    anchor-sections: true
    smooth-scroll: true
    hypothesis: true
  pdf:
    documentclass: scrreprt # scrbook
    keep-tex: true
    include-in-header: 
      text: |
        \usepackage{makeidx}
        \makeindex
    include-after-body: 
      text: |
        \printindex
  epub:
    cover-image: "pictures/R_delavnica_naslovna_slika.png"
  docx:
    reference-doc: custom-reference.docx

editor: source
---

`Bertopic` is a `Python` package for `topic modeling` that leverages the semantic capabilities of large language models (LLMs). My `Quarto` document shows how `bertopic` functions can be converted from `Python` to `R` code with the marvelous `reticulate` package. If you are more comfortable with coding in `R` than in `Python`, then this tutorial might be useful for you.

The programming steps of this tutorial were supported by the programming skills of `OpenAI`'s language model `GPT4o`. 


## Env & Packages

*Load* the `R` packages below and *initialize* a `Python` environment with the `reticulate` package.

By default, `reticulate` uses an isolated `Python` *virtual environment* named `r-reticulate` (cf. https://rstudio.github.io/reticulate/).

The `use_python()` function below enables you to specify an alternate `Python` (cf. https://rstudio.github.io/reticulate/).

```{r}
library(tidyverse)
library(tidytext)
library(quanteda)
library(tictoc)
library(htmltools)
library(htmlwidgets)
library(arrow)

library(reticulate)
# ~ Home, in cmd terminal insert: echo %USERPROFILE%
use_python("~/anaconda3/envs/bertopic", required = TRUE)
reticulate::py_config()
reticulate::py_available()

```

Load `BERTopic`-`r` functions for creating dataframes. The functions presuppose that a number of `Python` packages are already installed in the chosen `Python` environment (`BERTopic, numpy, plotly`, ...) and that the `R` packages above are installed and loaded.

```{r}
# source("functions/get_topic_df_1.R")
source("functions/find_topics_df.R")
source("functions/get_topic_df.R")
source("functions/get_topics_df.R")
source("functions/get_document_info_df.R")
source("functions/get_topic_info_df.R")
source("functions/get_representative_docs_custom.R")
source("functions/get_most_representative_docs.R")

source("functions/visualize_barchart.R")
# assuming that the 'topics_over_time' model and 'timestamps' are ready
source("functions/visualize_topics_over_time.R")
source("functions/visualize_distribution.R")
# optional hierarchical topics df
source("functions/visualize_hierarchy.R")
# define tpics_per_class beforehand
source("functions/visualize_topics_per_class.R")
source("functions/visualize_documents.R")

```


## Text preparation

You need the `arrow` package to open the `parquet` file below. 

The German texts are in the *text_clean* column. They were segmented into smaller chunks (each about 100 tokens long) to optimize topic extraction with `bertopic`. Special characters were removed with a cleaning function. The text chunks are all in lower case. 

```{r}
input_file = "data/wm_rr_lyrics_column_selection.parquet"
dataset <- read_parquet(input_file)
names(dataset)
dim(dataset)

```

The collected `stopword` list includes German and English tokens and will be inserted into `Python`'s `CountVectorizer` before `TF-IDF` calculation. The language model that will be used for the creation of embeddings will be fed with the text chunks in the *text_clean* column before `stopword` removal.

```{r}
input_file <- "stopwords/all_stopwords.txt"
all_stopwords <- read_lines(input_file)
```

Below are the lists of *texts_cleaned* and *timesteps*, which we need during the model preparation, topic extraction and visualization.

```{r}
texts_cleaned = dataset$text_clean
titles = dataset$doc_id
# timestamps <- as.list(dataset$date)
timestamps <- as.integer(dataset$year)

texts_cleaned[[1]]
```


## Model Preparation

Below are two sections that allow to choose between topic model preparation with *local* language models (via `ollama` or `lm-studio`) or with *online* language models hosted by `Groq`.

In both cases the `OpenAI` endpoint is used to connect the languages models.

By default, local models are enabled in this section with the `yaml` setting to *true*). You can change that: Set the `yaml` option `eval` in the `r` program chunk to *false* or *true* as needed.


### Local models

Before running the program chunk below, make sure to *install* all the necessary `Python` packages: `bertopic, numpy, scikit-learn, sentence-transformers, umap-learn, hdbscan, spacy, openai` and `datetime`. Download and install `ollama` or `lm-studio` to serve the *local* language model. 

In `RStudio`, you can accomplish this with `reticulate` commands: e.g., `reticulate::py_install ("bertopic", envname = "CHOOSE THE PATH YOUR PYTHON ENVIRONMENT")`. 

If you are familiar working with a (windows or conda) `terminal`, you can activate the appropriate `Python` or `Conda` environment and install the necessary packages: e.g., `pip install bertopic`).

First, we `import` the necessary `Python` packages: `numpy, umap, hdbscan, scikit-learn, sentence_transformers, and bertopic`.

`SentenceTransformer` creates the necessary *embeddings* for topic modeling with `bertopic`. The first time `SentenceTransformer` is used with a specific model, the model has to been downloaded from the `huggingface` website (https://huggingface.co/), where many freely usable language models are hosted (https://huggingface.co/models). 

In the next two steps, the `umap` module reduces the number of dimensions of embeddings, and the `hdbscan` module extracts clusters that can evaluated by the topic pipeline.

The `Countvectorizer` calculates the `c-TF-IDF` frequencies and enables the representation model defined below to extract suitable keywords as descriptors of the extracted topics.

In the example below, *multiple representation models* are used for keyword extraction from the identified topics and topic description: `keyBERT` (part of `bertopic`), a language model (served locally by `ollama` or online by `Groq`, both via the `OpenAI` endpoint), a Maximal Marginal Relevance model (`MMR`) and a `spacy` `POS` representation model. By default, only one Representation model is created by `bertopic`.

The *prompt* describes the task the language model has to accomplish, mentions the documents to work with and the topic labels that it should derive from the text contents and keywords.

`Bertopic` enables us to define a `zeroshot` list of keywords that can be used to drive the topic model towards desired topic outcomes. In the code below, the zeroshot keyword search is disabled, but can be activated if needed.

After all these preparational steps, the topic model is trained with:  `topic_model$fit_transform(texts, embeddings)`.

We obtain the *topic labels* with `topics <- fit_transform[[1]]` and the *topic probabilities* with `probs <- fit_transform[[2]]`.

The *topic labels* and *probabilities* are stored in a dataframe named *results*, together with other variables and metadata.

Since our dataset contains time-related metadata, we can use the `timestamps` for `dynamic topic modeling`, i.e., for discovering topic development or topic sequences through time. If your data doesn't contain any time-related column, the timestamps and topics_over_time calculations have to be disabled.

```{r}
#| eval: true

# Import necessary Python modules
np <- import("numpy")
umap <- import("umap")
UMAP <- umap$UMAP
hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN
sklearn <- import("sklearn")
CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer
bertopic <- import("bertopic")

# Embed the sentences
py <- import_builtins()
sentence_transformers <- import("sentence_transformers")
SentenceTransformer <- sentence_transformers$SentenceTransformer
embedding_model = SentenceTransformer("BAAI/bge-m3")
embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)

# Initialize UMAP and HDBSCAN models
umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)
hdbscan_model <- HDBSCAN(min_cluster_size=23L, min_samples = 17L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)

# Initialize CountVectorizer
vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L), 
                                    max_features = 10000L, max_df = 40L,
                                    stop_words = all_stopwords)
sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned)
sentence_vectors_dense <- np$array(sentence_vectors)
sentence_vectors_dense <- py_to_r(sentence_vectors_dense)

# Initialize representation models
keybert_model <- bertopic$representation$KeyBERTInspired()
openai <- import("openai")
OpenAI <- openai$OpenAI
ollama <- import("ollama")

# Point to the local server (ollama or lm-studio)
client <- OpenAI(base_url = 'http://localhost:11434/v1', api_key='ollama')
# client <- OpenAI(base_url = 'http://localhost:1234/v1', api_key='lm-studio')

prompt <- "
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]

Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
topic: <topic label>
"

# download & insert an appropriate local LLM for your computer with ollama/lm-studio
openai_model <- bertopic$representation$OpenAI(client, 
                                               model = "llama3.1:8b-instruct-fp16",
                                               exponential_backoff = TRUE, 
                                               chat = TRUE, 
                                               prompt = prompt)

# download a spacy language model from spacy.io and insert the model of your choice
pos_model <- bertopic$representation$PartOfSpeech("de_core_news_lg")
# diversity set relatively high to reduce repetition of keyword word forms
mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)

# Combine all representation models
representation_model <- list(
  "KeyBERT" = keybert_model,
  "OpenAI" = openai_model,
  "MMR" = mmr_model,
  "POS" = pos_model
)

# We can define a number of topics that we know are in the documents
# This option is disabled below
zeroshot_topic_list  <- list("german national identity", "minority issues in germany")

# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
  embedding_model = embedding_model,
  umap_model = umap_model,
  hdbscan_model = hdbscan_model,
  vectorizer_model = vectorizer_model,
  # zeroshot_topic_list = zeroshot_topic_list,
  # zeroshot_min_similarity = 0.85,
  representation_model = representation_model,
  calculate_probabilities = TRUE,
  top_n_words = 10L,
  verbose = TRUE
)

tictoc::tic()

# Train the model and transform the texts
fit_transform <- topic_model$fit_transform(texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# probs <- fit_transform[[2]]
# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities

tictoc::toc()


# Converting R Date to Python datetime
datetime <- import("datetime")

# timestamps <- as.list(dataset$date)
timestamps <- as.integer(dataset$year)

# # Convert each R date object to an ISO 8601 string
# timestamps <- lapply(timestamps, function(x) {
#   format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
# })

# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)


# Combine results with additional columns
results <- dataset |> 
  mutate(Topic = topics, 
         # Assuming the highest score for each sentence / text chunk
         Probability = apply(probs, 1, max)) 

results <- results |> 
  mutate(row_id = row_number()) |> 
  select(row_id, everything())

results |> 
  select(genre, artist, title, year, Topic, Probability, text_clean) |> 
  filter(Topic == 3) |> 
  head() |> 
  rmarkdown::paged_table()

# results |>
#   saveRDS("data/wm_topic_results_df_02.rds")
# results |>
#   write_csv("data/wm_topic_results_df_02.csv")
# results |>
#   writexl::write_xlsx("data/wm_topic_results_df_02.xlsx")

```


### Groq models

The model preparation chunk with online models hosted by `Groq` is disabled. Enable if needed (set `eval` to *true*). 

The `custom fit_transform_with_retry()` function usually is not necessary with local language models, but can be very helpful with online models served by Groq or other providers to avoid error messages due to *rate limits* or other workflow disturbancies.

```{r}
#| eval: false

# Import necessary Python modules
np <- import("numpy")
umap <- import("umap")
UMAP <- umap$UMAP

hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN

sklearn <- import("sklearn")
CountVectorizer <- sklearn$feature_extraction$text$CountVectorizer

bertopic <- import("bertopic")

# Embed the sentences
py <- import_builtins()
sentence_transformers <- import("sentence_transformers")
SentenceTransformer <- sentence_transformers$SentenceTransformer
# embedding_model = SentenceTransformer("intfloat/multilingual-e5-large")
embedding_model = SentenceTransformer("BAAI/bge-m3")
embeddings = embedding_model$encode(texts_cleaned, show_progress_bar=TRUE)

# Initialize UMAP and HDBSCAN models
umap_model <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)
hdbscan_model <- HDBSCAN(min_cluster_size=23L, min_samples = 17L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)

# Initialize CountVectorizer
vectorizer_model <- CountVectorizer(min_df=2L, ngram_range=tuple(1L, 3L), 
                                    max_features = 10000L, max_df = 40L,
                                    stop_words = all_stopwords)
sentence_vectors <- vectorizer_model$fit_transform(texts_cleaned)
sentence_vectors_dense <- np$array(sentence_vectors)
sentence_vectors_dense <- py_to_r(sentence_vectors_dense)

# Initialize representation models
keybert_model <- bertopic$representation$KeyBERTInspired()
openai <- import("openai")
OpenAI <- openai$OpenAI

os <- import("os")
groq <- import("groq")
dotenv <- import("dotenv")
load_dotenv <- dotenv$load_dotenv
dotenv_path <- '.env'
load_dotenv(dotenv_path)

# groq_api_key <- os$environ['GROQ_API_KEY']
# client <- groq$Groq(api_key=groq_api_key)

client = OpenAI(
  base_url="https://api.groq.com/openai/v1",
  api_key=os$environ$get("GROQ_API_KEY")
  )

prompt <- "
I have a topic that contains the following documents:
[DOCUMENTS]
The topic is described by the following keywords: [KEYWORDS]

Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:
topic: <topic label>
"

openai_model <- bertopic$representation$OpenAI(client, 
                                               # model = "llama3-70b-8192", 
                                               # model = "mixtral-8x7b-32768"
                                               model = "llama-3.1-70b-versatile",
                                               exponential_backoff = TRUE, 
                                               chat = TRUE, 
                                               prompt = prompt)

pos_model <- bertopic$representation$PartOfSpeech("de_core_news_lg")
mmr_model <- bertopic$representation$MaximalMarginalRelevance(diversity = 0.5)

# Combine all representation models
representation_model <- list(
  "KeyBERT" = keybert_model,
  "OpenAI" = openai_model,
  "MMR" = mmr_model,
  "POS" = pos_model
)

# We define a number of topics that we know are in the documents
zeroshot_topic_list  <- list("german national identity", "minority issues in germany")

# Initialize BERTopic model with pipeline models and hyperparameters
BERTopic <- bertopic$BERTopic
topic_model <- BERTopic(
  embedding_model = embedding_model,
  umap_model = umap_model,
  hdbscan_model = hdbscan_model,
  vectorizer_model = vectorizer_model,
  # zeroshot_topic_list = zeroshot_topic_list,
  # zeroshot_min_similarity = 0.85,
  representation_model = representation_model,
  calculate_probabilities = TRUE,
  top_n_words = 10L,
  verbose = TRUE
)

tictoc::tic()

# Define a function to fit the model and transform texts with error handling
fit_transform_with_retry <- function(topic_model, texts, embeddings, max_retries = 5, wait_time = 60) {
  attempt <- 1
  success <- FALSE
  result <- NULL
  
  while (!success && attempt <= max_retries) {
    tryCatch(
      {
        # Attempt to fit the model and transform texts
        fit_transform <- topic_model$fit_transform(texts, embeddings)
        success <- TRUE  # Mark success if no error occurs
        result <- fit_transform
      },
      error = function(e) {
        message("Error occurred: ", conditionMessage(e))
        if (attempt < max_retries) {
          message("Retrying in ", wait_time, " seconds... (Attempt ", attempt, " of ", max_retries, ")")
          Sys.sleep(wait_time)  # Wait before retrying
          attempt <<- attempt + 1  # Increment attempt counter
        } else {
          stop("Max retries reached. Unable to complete request.")
        }
      }
    )
    
    if (success) {
      break  # Exit the loop if the operation is successful
    }
  }
  
  return(result)
}

# Use the function to fit the model and transform the texts
fit_transform <- fit_transform_with_retry(topic_model, texts_cleaned, embeddings)
topics <- fit_transform[[1]]
# Now transform the texts to get the updated probabilities
transform_result <- topic_model$transform(texts_cleaned)
probs <- transform_result[[2]]  # Extract the updated probabilities

tictoc::toc()


# Converting R Date to Python datetime
datetime <- import("datetime")

# timestamps <- as.list(dataset$date)
timestamps <- as.integer(dataset$year)

# # Convert each R date object to an ISO 8601 string
# timestamps <- lapply(timestamps, function(x) {
#   format(x, "%Y-%m-%dT%H:%M:%S")  # ISO 8601 format
# })

# Dynamic topic model
topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)


# Combine results with additional columns
results <- dataset |> 
  mutate(Topic = topics, 
         Probability = apply(probs, 1, max))  # Assuming the highest probability for each sentence

results <- results |> 
  mutate(row_id = row_number()) |> 
  select(row_id, everything())

head(results)

# results |>
#   saveRDS("data/wm_topic_results_df_01.rds")
# results |>
#   write_csv("data/wm_topic_results_df_01.csv")
# results |>
#   writexl::write_xlsx("data/wm_topic_results_df_01.xlsx")
```


## Results

After model training, we are in the position to inspect the results of the obtained `topic model` we simply named *topic_model*. 

### Number of topics

Check how many topics were extracted from `Bertopic`'s function `get_topic_info()`. The number of topics includes the `outlier` topic (-1), which usually is removed from analysis.

```{r}
# Get topic information
topic_info_df <- topic_model$get_topic_info()
# number of topics (-1 included)
print(paste("Number of topics: ", length(topic_info_df$Topic)))
```

### Topic frequencies

There are several `bertopic` functions to get to know the results of the topic analysis, e.g., `get_topic_freq()` to obtain a table with topic frequencies.

```{r}
topic_model$get_topic_freq() |> head(10)
```

The table shows how many documents (or in our case: how many text chunks) are attributed to a certain topic. The topics are sorted by their frequencies.

Topic (-1) represents document (text chunks) that could not be attributed to any of the extracted topics. `Bertopic` has the option to reduce the number of *outliers* (cf. the section about Outlier reduction).

### Topic search

The `find_topics()` function enables the user to input a `query` and to search for topics that are `semantically similar` to our query. In the example below, we are searching for ten topics (`top_n`=*10L*) that are similar to our `query`.

The result is returned in `list` format, but we can make a `data.frame` that contains the relevant `topics` and their `similarity scores`.

```{r}
query <- "einwanderer"
similar_topics <- topic_model$find_topics(query, top_n=10L)

similar_topics_df <- data.frame(
  keyword = query,
  topics = similar_topics[[1]],
  similarity = unlist(similar_topics[[2]])
)

similar_topics_df
```

We can make a simple custom function find_topics_df(), which creates a dataframe with the most similar topics.

```{r}
find_topics_df <- function(model=topic_model, query, top_n=10L){
  similar_topics <- topic_model$find_topics(query, top_n=10L)
  similar_topics_df <- data.frame(
    keyword = query,
    topics = similar_topics[[1]],
    similarity = unlist(similar_topics[[2]])
  )
  return(similar_topics_df)
}

find_topics_df(topic_model, "kleines wunder", top_n=5L)
```

Improved `get_topics_df()` function (with error handling and integer handling):

```{r}
#' Find Topics DataFrame Function
#'
#' This function finds the most similar topics to a given keyword using a BERTopic model
#' and returns the results in a data frame or tibble format.
#'
#' @param model A BERTopic model object. Must be passed from the calling environment.
#' @param query A keyword to query the topics for.
#' @param top_n Number of top similar topics to retrieve. Default is 10.
#' @param return_tibble Logical. If TRUE, returns a tibble. If FALSE, returns a data.frame. Default is TRUE.
#' @return A data.frame or tibble with columns for the keyword, topics, and similarity scores.
#' @examples
#' find_topics_df(model = topic_model, query = "liebe", top_n = 10)
#'
find_topics_df <- function(model, query, top_n = 10, return_tibble = TRUE) {
  # Check if the model and query are provided
  if (missing(model)) {
    stop("A BERTopic model must be provided.")
  }
  if (missing(query)) {
    stop("A query keyword must be provided.")
  }
  
  # Find similar topics using the model
  similar_topics <- model$find_topics(query, top_n = as.integer(top_n))
  
  # Create the data frame
  similar_topics_df <- data.frame(
    keyword = query,
    topics = similar_topics[[1]],
    similarity = unlist(similar_topics[[2]])
  )
  
  # Return as tibble if specified
  if (return_tibble) {
    similar_topics_df <- as_tibble(similar_topics_df)
  }
  
  return(similar_topics_df)
}

# Example usage
find_topics_df(model = topic_model, query = "migranten", top_n = 20)

```


Now we can have a closer look at an example of a topic about our query by using the get_topic_info() function with the relevant topic number. 

In this example, `Topic` *3* shows a higher `Similarity` score. The `Count` shows that 123 text chunks were attributed to this topic. The `Name` of the topic also serves as topic identifier and contains the concatenated topic number and multiple keywords of the topic. 

```{r}
topic_model$get_topic_info(3)
```

We can also see that the `columns` of the `representation models` cannot be accessed yet as these `columns` are still in `list` format and have to be `unnested` (see more about that in the next section below).

But before that, let us have another `query` example, but this time with *multiple phrases*. Though possible, it can become quickly quite complicated to unveil the search results.

```{r}
queries <- c("national minority", "minority issues", "nationality issues")
search_results <- lapply(queries, function(query) topic_model$find_topics(query))

# Accessing the results
search_results[[1]][[1]]  # Topics for "national minority"
search_results[[2]][[1]]  # Topics for "minority issues"
search_results[[3]][[1]]  # Topics for "nationality issues"

```

*Improved* function `get_topics_df()`, which can handle multiple queries, includes error and integer handling, and returns a dataframe with top_n topics for each query:

```{r}
#' Find Topics DataFrame Function
#'
#' This function finds the most similar topics to given keywords using a BERTopic model
#' and returns the results in a data frame or tibble format.
#'
#' @param model A BERTopic model object. Must be passed from the calling environment.
#' @param queries A vector of keywords or phrases to query the topics for.
#' @param top_n Number of top similar topics to retrieve for each query. Default is 10.
#' @param return_tibble Logical. If TRUE, returns a tibble. If FALSE, returns a data.frame. Default is TRUE.
#' @return A data.frame or tibble with columns for the keyword, topics, and similarity scores for each query.
#' @examples
#' queries <- c("national minority", "minority issues", "nationality issues")
#' find_topics_df(model = topic_model, queries = queries, top_n = 10)
#'
find_topics_df <- function(model, queries, top_n = 10, return_tibble = TRUE) {
  # Check if the model and queries are provided
  if (missing(model)) {
    stop("A BERTopic model must be provided.")
  }
  if (missing(queries)) {
    stop("A vector of queries must be provided.")
  }
  
  # Ensure top_n is an integer
  top_n <- as.integer(top_n)
  
  # Initialize an empty list to store results
  results_list <- list()
  
  # Loop through each query and retrieve the top topics
  for (query in queries) {
    similar_topics <- model$find_topics(query, top_n = top_n)
    
    # Create a temporary data frame for the current query
    similar_topics_df <- data.frame(
      keyword = query,
      topics = similar_topics[[1]],
      similarity = unlist(similar_topics[[2]])
    )
    
    # Append the result to the results list
    results_list[[query]] <- similar_topics_df
  }
  
  # Combine all results into a single data frame
  combined_results_df <- do.call(rbind, results_list)
  
  # Return as tibble if specified
  if (return_tibble) {
    combined_results_df <- as_tibble(combined_results_df)
  }
  
  return(combined_results_df)
}

# Example usage
queries <- "heimat"
queries <- c("national minority", "minority issues", "nationality issues")
search_results <- find_topics_df(model = topic_model, queries = queries, top_n = 3)
search_results |> rmarkdown::paged_table()
```

The saved function can be loaded like this:

```{r}
source("functions/find_topics_df.R")

# Example usage
find_topics_df(topic_model, c("vaterland", "heimat"), 3)
```


### Topic word list

To *view* the *keywords* of a selected topic and their scores, use the `get_topic()` function. Again, in list format, the result of the `Python` function has to be converted to `data.frame` format. 

```{r}
topic_selected <- topic_model$get_topic(3)

# Convert to a DataFrame
topic_selected_df <- as.data.frame(do.call(rbind, topic_selected), 
                                   stringsAsFactors = FALSE)
# Set column names
colnames(topic_selected_df) <- c("Word", "Score")

topic_selected_df <- topic_selected_df |> 
  unnest(Word) |> 
  unnest(Score)

# View the DataFrame
topic_selected_df
```


We can make a custom function `get_topic_df()` to obtain a dataframe to display ten words with high probability for the chosen topic. 

```{r}
# Get the topic list for a specific topic number
get_topic_df <- function(model=topic_model, topic_number=0) {
  topic_wordlist <- topic_model$get_topic(as.integer(topic_number))
  words <- map_chr(topic_wordlist, ~ as.character(.[[1]]))
  scores <- map_dbl(topic_wordlist, ~ .[[2]])
  data.frame(Word = words, Score = scores, Topic = as.integer(topic_number))
}

# Convert the topic list to a DataFrame
topic_df <- get_topic_df(topic_model, 3)
topic_df
```

*Improved* `get_topic_df()` function (with customizable number of words, error and integer handling):

```{r}
#' Get Topic DataFrame Function
#'
#' This function retrieves a specified number of words with high probability for a given topic number
#' from a BERTopic model and returns the results in a data frame or tibble format.
#'
#' @param model A BERTopic model object. Must be passed from the calling environment.
#' @param topic_number The topic number for which words and scores are retrieved.
#' @param top_n Number of top words to retrieve for the specified topic. Default is 10.
#'              If greater than 10, it will be set to 10 as BERTopic returns a maximum of 10 words.
#' @param return_tibble Logical. If TRUE, returns a tibble. If FALSE, returns a data.frame. Default is TRUE.
#' @return A data.frame or tibble with columns for the word, score, and topic number.
#' @examples
#' get_topic_df(model = topic_model, topic_number = 3, top_n = 5)
#'
get_topic_df <- function(model, topic_number = 0, top_n = 10, return_tibble = TRUE) {
  # Check if the model is provided
  if (missing(model)) {
    stop("A BERTopic model must be provided.")
  }
  
  # Check if the topic number is provided and is a non-negative integer
  if (!is.numeric(topic_number) || topic_number < 0) {
    stop("A valid topic number must be provided (non-negative integer).")
  }
  
  # Ensure top_n is an integer and not more than 10
  top_n <- as.integer(top_n)
  if (top_n > 10) {
    warning("top_n is greater than 10. BERTopic returns a maximum of 10 words; setting top_n to 10.")
    top_n <- 10
  }
  
  # Retrieve the topic word list from the model
  topic_wordlist <- model$get_topic(topic_number)
  
  # Check if the topic_wordlist is valid
  if (is.null(topic_wordlist) || length(topic_wordlist) == 0) {
    stop(paste("No words found for topic number", topic_number))
  }
  
  # Limit the results to top_n words
  words <- map_chr(topic_wordlist[1:top_n], ~ as.character(.[[1]]))
  scores <- map_dbl(topic_wordlist[1:top_n], ~ .[[2]])
  
  # Create the data frame
  topic_df <- data.frame(
    Word = words,
    Score = scores,
    Topic = as.integer(topic_number)
  )
  
  # Return as tibble if specified
  if (return_tibble) {
    topic_df <- as_tibble(topic_df)
  }
  
  return(topic_df)
}

# Example usage
topic_df <- get_topic_df(model = topic_model, topic_number = 3, top_n = 12)
print(topic_df)

# Example usage
topic_df <- get_topic_df(model = topic_model, topic_number = 3, top_n = 5)
print(topic_df)

```

We can load the `get_topic_df()` function from the `functions` folder:

```{r}
source("functions/get_topic_df.R")

get_topic_df(topic_model, 3, 5)
```

```{r}
#| eval: false
topic_model$get_topics()
```



Use the custom function below to obtain ten most probable words per topic in a dataframe:

```{r}
# Ensure `topic_df` is initialized as a data frame
topics_df <- data.frame()

# Get unique topics and sort them
topics_unique <- unique(topics)
topics_unique <- sort(topics_unique)
# topics_unique <- topics_unique[topics_unique != -1]

convert_topic_list_to_df <- function(topic_list) {
  words <- map_chr(topic_list, ~ as.character(.[[1]]))
  scores <- map_dbl(topic_list, ~ .[[2]])
  data.frame(Word = words, Score = scores)
}

# Loop over each unique topic, skipping -1
for (i in seq_along(topics_unique)) {
  topic_number <- topics_unique[i]  # Get the current topic number
  # Skip the -1 topic
  if (topic_number == -1) {
    next
  }
  topic_list <- topic_model$get_topic(topic_number)  # Retrieve the topic list
  if (length(topic_list) > 0) {  # Check if the topic list is not empty
    df <- convert_topic_list_to_df(topic_list)  # Convert to a DataFrame
    df$Topic <- topic_number  # Add the topic number to the DataFrame
    topics_df <- rbind(topics_df, df)  # Append to the main DataFrame
  }
}

# Display the resulting DataFrame
topics_df |> rmarkdown::paged_table()

```

As another alternative: Ten most probable words per topic in a dataframe - derived from a bertopic dictionary.

```{r}
# Get the topics dictionary
topics_dict <- topic_model$get_topics()

# Initialize an empty DataFrame
topics_df <- data.frame()

# Loop through the topics dictionary and convert each topic to a DataFrame
for (topic_number in names(topics_dict)) {
  topic_list <- topics_dict[[topic_number]]
  if (length(topic_list) > 0) {
    df <- convert_topic_list_to_df(topic_list)
    df$Topic <- as.numeric(topic_number)
    topics_df <- rbind(topics_df, df)
  }
}

# Display the resulting DataFrame
topics_df |> rmarkdown::paged_table()

```

Custom function `get_topics_df()`:

```{r}
get_topics_df <- function(model=topic_model){
  # Initialize an empty DataFrame
  topics_df <- data.frame()
  # Get the topics dictionary
  topics_dict <- topic_model$get_topics()
  # Loop through the topics dictionary and convert each topic to a DataFrame
  for (topic_number in names(topics_dict)) {
    topic_list <- topics_dict[[topic_number]]
    if (length(topic_list) > 0) {
      df <- convert_topic_list_to_df(topic_list)
      df$Topic <- as.numeric(topic_number)
      topics_df <- rbind(topics_df, df)
    }
  }
  return(topics_df)
}

get_topics_df(topic_model)

```

Improved `get_topics_df()` function:

```{r}
#' Get Topics DataFrame Function
#'
#' This function retrieves all topics from a BERTopic model and converts them into a data frame or tibble format.
#'
#' @param model A BERTopic model object. Must be passed from the calling environment.
#' @param return_tibble Logical. If TRUE, returns a tibble. If FALSE, returns a data.frame. Default is TRUE.
#' @return A data.frame or tibble with columns for the word, score, and topic number across all topics.
#' @examples
#' get_topics_df(model = topic_model)
#'
get_topics_df <- function(model, return_tibble = TRUE) {
  # Check if the model is provided
  if (missing(model)) {
    stop("A BERTopic model must be provided.")
  }
  
  # Initialize an empty list to store DataFrames for each topic
  topics_list <- list()
  
  # Get the topics dictionary
  topics_dict <- model$get_topics()
  
  # Check if topics_dict is valid and has content
  if (is.null(topics_dict) || length(topics_dict) == 0) {
    stop("No topics found in the provided model.")
  }
  
  # Loop through the topics dictionary and convert each topic to a DataFrame
  for (topic_number in names(topics_dict)) {
    topic_list <- topics_dict[[topic_number]]
    if (length(topic_list) > 0) {
      df <- convert_topic_list_to_df(topic_list)
      df$Topic <- as.numeric(topic_number)
      topics_list[[length(topics_list) + 1]] <- df
    }
  }
  
  # Combine all DataFrames into one
  topics_df <- do.call(rbind, topics_list)
  
  # Return as tibble if specified
  if (return_tibble) {
    topics_df <- as_tibble(topics_df)
  }
  
  return(topics_df)
}

# Example usage
topics_df <- get_topics_df(model = topic_model)

```


Load the custom `get_topics_df()` function from the `functions` folder:

```{r}
source("functions/get_topics_df.R")
get_topics_df(topic_model)
```

Visualize *top five words* in an interactive `plotly` `barchart` with the function `visualize_barchart()` of `bertopic`, which uses the 7 package `plotly` behind the scenes. This is the reason why we do not load the `R` `plotly` package.

```{r}
plotly <- import("plotly")
np <- import("numpy")

# Visualize topics
fig <- topic_model$visualize_barchart(custom_labels=FALSE)

# # Render the plot using plotly
# plotly$offline$iplot(fig)

# Save the figure as an HTML file
plotly$offline$plot(fig, filename = "wm_topics_topwords_interactive.html", 
                    auto_open = TRUE)

library(htmltools)
library(htmlwidgets)
# Read the HTML file content as a single string
html_content <- read_file(
  "wm_topics_topwords_interactive.html")
# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```

Custom interactive plotly barchart:

```{r}
visualize_barchart <- function(model=topic_model){
  plotly <- import("plotly")
  np <- import("numpy")
  # Visualize topics
  fig <- topic_model$visualize_barchart(custom_labels=FALSE)
  # Save the figure as an HTML file
  plotly$offline$plot(fig, filename = "topics_topwords_interactive_barchart.html", 
                      auto_open = TRUE)
  library(htmltools)
  library(htmlwidgets)
  # Read the HTML file content as a single string
  html_content <- read_file(
    "topics_topwords_interactive_barchart.html")
  # Display the saved HTML file content in the Quarto notebook
  browsable(HTML(html_content))  
}

visualize_barchart(topic_model)
```


Improved `visualize_barchart()` function:

```{r}
#' Visualize BERTopic Bar Chart
#'
#' This function visualizes the topics of a BERTopic model using Plotly and saves the output
#' as an interactive HTML file. It checks for required Python modules and allows for custom file naming.
#'
#' @param model A BERTopic model object. Must be passed from the calling environment.
#' @param filename A character string specifying the name of the HTML file to save the bar chart.
#'                 Default is "topics_topwords_interactive_barchart.html".
#' @param open_file Logical. If TRUE, opens the HTML file after saving. Default is TRUE.
#' @return Displays the interactive bar chart within the R environment and saves it as an HTML file.
#' @examples
#' visualize_barchart(model = topic_model, filename = "custom_barchart.html", open_file = TRUE)
#'
visualize_barchart <- function(model, filename = "topics_topwords_interactive_barchart.html", open_file = FALSE) {
  # Check if the model is provided
  if (missing(model)) {
    stop("A BERTopic model must be provided.")
  }
  
  # Import necessary Python modules using reticulate
  plotly <- tryCatch(import("plotly"), error = function(e) {
    stop("Python module 'plotly' is not installed. Please install it using pip.")
  })
  
  np <- tryCatch(import("numpy"), error = function(e) {
    stop("Python module 'numpy' is not installed. Please install it using pip.")
  })
  
  # Visualize topics using BERTopic's visualize_barchart function
  fig <- model$visualize_barchart(custom_labels = FALSE)
  
  # Save the figure as an HTML file
  plotly$offline$plot(fig, filename = filename, auto_open = open_file)
  
  # Load necessary R libraries
  if (!requireNamespace("htmltools", quietly = TRUE)) {
    stop("The 'htmltools' package is not installed. Please install it using install.packages('htmltools').")
  }
  
  if (!requireNamespace("htmlwidgets", quietly = TRUE)) {
    stop("The 'htmlwidgets' package is not installed. Please install it using install.packages('htmlwidgets').")
  }
  
  # Use htmltools and htmlwidgets to display the HTML content in the notebook
  library(htmltools)
  library(htmlwidgets)
  
  # Read the HTML file content as a single string
  html_content <- read_file(filename)
  
  # Display the saved HTML file content
  browsable(HTML(html_content))
}

# Example usage
viz <- visualize_barchart(model = topic_model, filename = "custom_barchart.html", open_file = TRUE)

```

Load the custom visualize_barchart() function:

```{r}
source("functions/visualize_barchart.R")
visualize_barchart(topic_model, "interactive_custom_barchart.html", open_file = TRUE)
```


### Document table

The `get_document_info()` function produces a `data.frame` of each document (in our case: text chunk) and the associated categories and values: `Topic, Name, Representation, KeyBERT`, LLM (here labelled `OpenAI`), `MMR, POS`,  `Representative_Docs` and `Top_n_words`. 

```{r}
document_info_df <- topic_model$get_document_info(texts_cleaned)
document_info_df |> head() |> rmarkdown::paged_table()
```

Multiple columns in the table above have to be unnested before assessment is possible.

```{r}
unnested_doc_info_df1 <- document_info_df %>%
  unnest_wider(Representation, names_sep = "_") |> 
  unnest_wider(KeyBERT, names_sep = "_") |>
  unnest_wider(OpenAI, names_sep = "_") |>
  unnest_wider(MMR, names_sep = "_") |>
  unnest_wider(POS, names_sep = "_") |>
  unnest_wider(Representative_Docs, names_sep = "_")

# Replace NA with an empty string across all columns
unnested_doc_info_df1[is.na(unnested_doc_info_df1)] <- ""

# Unite columns that start with the same prefix
unnested_doc_info_df1 <- unnested_doc_info_df1 %>%
  rowwise() %>%  # Process row by row to handle NA and varying lengths
  mutate(Representation = paste(c_across(starts_with("Representation_")), collapse = ", "),
         KeyBERT = paste(c_across(starts_with("KeyBERT_")), collapse = ", "),
         OpenAI = paste(c_across(starts_with("OpenAI_")), collapse = ", "),
         MMR = paste(c_across(starts_with("MMR_")), collapse = ", "),
         POS = paste(c_across(starts_with("POS_")), collapse = ", "),
         Representative_Docs = paste(c_across(starts_with("Representative_Docs_")), collapse = ", ")) %>%
  ungroup()  # Un-group the rows after processing

# Drop the old columns (optional)
unnested_doc_info_df1 <- unnested_doc_info_df1 %>%
  select(-starts_with("Representation_"),
         -starts_with("KeyBERT_"),
         -starts_with("OpenAI_"),
         -starts_with("MMR_"),
         -starts_with("POS_"),
         -starts_with("Representative_Docs_"))

unnested_doc_info_df1 <- unnested_doc_info_df1 |> 
  mutate(seq_id = row_number())

# unnested_doc_info_df1 |>
#   # distinct(Name, .keep_all = TRUE) |>
#   saveRDS("data/wm_doc_topics_01.rds")
# unnested_doc_info_df1 |>
#   # distinct(Name, .keep_all = TRUE) |>
#   write_csv("data/wm_doc_topics_01.csv") |>
#   writexl::write_xlsx("data/wm_doc_topics_01.xlsx")
```

Below is a custom function to create a dataframe with unnested list columns. 
The custom `get_document_info_df()` function looks like this:

```{r}
#' Get Document Information DataFrame
#'
#' This function retrieves document information from a BERTopic model and processes it to unnest
#' list columns, replace NA values, and consolidate columns with the same prefix.
#'
#' @param model A BERTopic model object.
#' @param texts A character vector containing the preprocessed texts to be passed to the BERTopic model.
#' @param drop_expanded_columns Logical. If TRUE, drops the expanded columns after consolidation. Default is TRUE.
#' @return A data.frame or tibble with unnested and consolidated columns.
#' @examples
#' document_info_df <- get_document_info_df(model = topic_model, texts = texts_cleaned, drop_expanded_columns = TRUE)
#'
get_document_info_df <- function(model, texts, drop_expanded_columns = TRUE) {
  # Check if the model and texts are provided
  if (missing(model) || missing(texts)) {
    stop("Both a BERTopic model and texts must be provided.")
  }
  
  # Get document info from the BERTopic model
  document_info_df <- model$get_document_info(texts)
  
  # Load necessary libraries
  library(dplyr)
  library(tidyr)
  
  # Temporarily rename the "Representative_document" and "Representative_Docs" columns to avoid conflicts
  if ("Representative_document" %in% names(document_info_df)) {
    document_info_df <- document_info_df %>%
      rename(Is_Representative_document = Representative_document)
  }
  
  if ("Representative_Docs" %in% names(document_info_df)) {
    document_info_df <- document_info_df %>%
      rename(RepresentativeDocs = Representative_Docs)
  }
  
  # Identify list columns to unnest
  list_cols <- names(document_info_df)[sapply(document_info_df, is.list)]
  
  # Unnest the list columns using unnest_wider
  for (col in list_cols) {
    document_info_df <- document_info_df %>%
      unnest_wider(!!sym(col), names_sep = "_")
  }
  
  # Replace NA with an empty string across all columns
  document_info_df[is.na(document_info_df)] <- ""
  
  # Dynamically consolidate columns with the same prefix
  prefixes <- unique(gsub("_.*", "", list_cols))
  
  for (prefix in prefixes) {
    # General case for other prefixes
    prefix_cols <- grep(paste0("^", prefix, "_"), names(document_info_df), value = TRUE)
    document_info_df[prefix_cols] <- lapply(document_info_df[prefix_cols], as.character)
    document_info_df <- document_info_df %>%
      rowwise() %>%
      mutate(
        !!sym(prefix) := paste(c_across(all_of(prefix_cols)), collapse = ", ")
      ) %>%
      ungroup()
  }
  
  # Optionally drop the old expanded columns
  if (drop_expanded_columns) {
    for (prefix in prefixes) {
      prefix_cols <- grep(paste0("^", prefix, "_"), names(document_info_df), value = TRUE)
      if (length(prefix_cols) > 0) {
        document_info_df <- document_info_df %>%
          select(-all_of(prefix_cols))
      }
    }
  }
  
  # Revert the temporary column name changes back to original names
  if ("Is_Representative_document" %in% names(document_info_df)) {
    document_info_df <- document_info_df %>%
      rename(Representative_document = Is_Representative_document)
  }

  if ("RepresentativeDocs" %in% names(document_info_df)) {
    document_info_df <- document_info_df %>%
      rename(Representative_Docs = RepresentativeDocs)
  }
  
  return(document_info_df)
}

# # Example usage
# document_info_df <- get_document_info_df(model = topic_model, texts = texts_cleaned, drop_expanded_columns = TRUE)
# print(document_info_df)

```

Load the custom `get_document_info_df()` function from the `functions` folder:

```{r}
source("functions/get_document_info_df.R")

unnested_doc_info_df1 <- get_document_info_df(model = topic_model, texts = texts_cleaned, drop_expanded_columns = TRUE)

unnested_doc_info_df1 <- unnested_doc_info_df1 |> 
  mutate(seq_id = row_number())

unnested_doc_info_df1 |> 
  # filter(Topic == 0) |>
  # distinct(Name, .keep_all = TRUE) |> 
  head(3)

```


Now we can assess the results of each document (in our case: text chunk). 

```{r}
unnested_doc_info_df1 |> 
  filter(Topic == 3 & Representative_document == TRUE)
```


This table could be joined with our input data frame we named *dataset*, although we should check if the sq_ids are synchronized.

```{r}
df_joined <- dataset |> 
  # rename(Document = text_clean) |> 
  left_join(unnested_doc_info_df1, join_by(seq_id))

df_joined |> head(3) |> rmarkdown::paged_table()

# df_joined |>
#   saveRDS("data/wm_joined_doc_topics_01.rds")
# df_joined |>
#   write_csv("data/wm_joined_doc_topics_01.csv") |>
#   writexl::write_xlsx("data/wm_joined_doc_topics_01.xlsx")
```


### Topic table

The `get_topic_info()` function returns a data.frame that has multiple columns in list format. The following code unnests these columns.

```{r}
# Get topic information
topic_info_df <- topic_model$get_topic_info()

# Create the dataframe with unnested columns
unnested_topic_info_df1 <- topic_info_df %>%
  unnest_wider(Representation, names_sep = "_") |> 
  unnest_wider(KeyBERT, names_sep = "_") |>
  unnest_wider(OpenAI, names_sep = "_") |>
  unnest_wider(MMR, names_sep = "_") |>
  unnest_wider(POS, names_sep = "_") |>
  unnest_wider(Representative_Docs, names_sep = "_")

# Replace NA with an empty string across all columns
unnested_topic_info_df1[is.na(unnested_topic_info_df1)] <- ""

# Unite columns that start with the same prefix
unnested_topic_info_df <- unnested_topic_info_df1 %>%
  rowwise() %>%  # Process row by row to handle NA and varying lengths
  mutate(Representation = paste(c_across(starts_with("Representation_")), collapse = ", "),
         KeyBERT = paste(c_across(starts_with("KeyBERT_")), collapse = ", "),
         OpenAI = paste(c_across(starts_with("OpenAI_")), collapse = ", "),
         MMR = paste(c_across(starts_with("MMR_")), collapse = ", "),
         POS = paste(c_across(starts_with("POS_")), collapse = ", "),
         Representative_Docs = paste(c_across(starts_with("Representative_Docs_")), collapse = ", ")) %>%
  ungroup()  # Un-group the rows after processing

# Drop the old columns (optional)
unnested_topic_info_df <- unnested_topic_info_df %>%
  select(-starts_with("Representation_"),
         -starts_with("KeyBERT_"),
         -starts_with("OpenAI_"),
         -starts_with("MMR_"),
         -starts_with("POS_"),
         -starts_with("Representative_Docs_"))

# unnested_topic_info_df |>
#   # distinct(Name, .keep_all = TRUE) |>
#   saveRDS("data/wm_topics_01.rds")
# unnested_topic_info_df |>
#   # distinct(Name, .keep_all = TRUE) |>
#   write_csv("data/wm_topics_01.csv") |>
#   writexl::write_xlsx("data/wm_topics_01.xlsx")

```

As custom get_topic_info_df() function:

```{r}
#' Get Topic Information DataFrame
#'
#' This function retrieves topic information from a BERTopic model and processes it to unnest
#' list columns, replace NA values, and consolidate columns with the same prefix.
#'
#' @param model A BERTopic model object.
#' @param drop_expanded_columns Logical. If TRUE, drops the expanded columns after consolidation. Default is TRUE.
#' @return A data.frame or tibble with unnested and consolidated columns.
#' @examples
#' topic_info_df <- get_topic_info_df(model = topic_model, drop_expanded_columns = TRUE)
#'
get_topic_info_df <- function(model, drop_expanded_columns = TRUE) {
  # Check if the model is provided
  if (missing(model)) {
    stop("A BERTopic model must be provided.")
  }
  
  # Get topic info from the BERTopic model
  topic_info_df <- model$get_topic_info()
  
  # Load necessary libraries
  library(dplyr)
  library(tidyr)
  
  # Temporarily rename conflicting columns if they exist
  if ("Representative_Docs" %in% names(topic_info_df)) {
    topic_info_df <- topic_info_df %>%
      rename(RepresentativeDocs = Representative_Docs)
  }
  
  # Identify list columns to unnest
  list_cols <- names(topic_info_df)[sapply(topic_info_df, is.list)]
  
  # Unnest the list columns using unnest_wider
  for (col in list_cols) {
    topic_info_df <- topic_info_df %>%
      unnest_wider(!!sym(col), names_sep = "_")
  }
  
  # Replace NA with an empty string across all columns
  topic_info_df[is.na(topic_info_df)] <- ""
  
  # Dynamically consolidate columns with the same prefix
  prefixes <- unique(gsub("_.*", "", list_cols))
  
  for (prefix in prefixes) {
    # General case for other prefixes
    prefix_cols <- grep(paste0("^", prefix, "_"), names(topic_info_df), value = TRUE)
    topic_info_df[prefix_cols] <- lapply(topic_info_df[prefix_cols], as.character)
    topic_info_df <- topic_info_df %>%
      rowwise() %>%
      mutate(
        !!sym(prefix) := paste(c_across(all_of(prefix_cols)), collapse = ", ")
      ) %>%
      ungroup()
  }
  
  # Optionally drop the old expanded columns
  if (drop_expanded_columns) {
    for (prefix in prefixes) {
      prefix_cols <- grep(paste0("^", prefix, "_"), names(topic_info_df), value = TRUE)
      if (length(prefix_cols) > 0) {
        topic_info_df <- topic_info_df %>%
          select(-all_of(prefix_cols))
      }
    }
  }
  
  # Revert the temporary column name changes back to original names
  if ("RepresentativeDocs" %in% names(topic_info_df)) {
    topic_info_df <- topic_info_df %>%
      rename(Representative_Docs = RepresentativeDocs)
  }
  
  return(topic_info_df)
}

# Example usage
topic_info_df <- get_topic_info_df(model = topic_model, drop_expanded_columns = TRUE)
head(topic_info_df)

```


Load the custom `get_topic_info_df()` from the `functions` folder:

```{r}
source("functions/get_topic_info_df.R")
# Example usage
topic_info_df <- get_topic_info_df(model = topic_model, drop_expanded_columns = TRUE)
head(topic_info_df)

```



In the following program chunks, we join the data frames, first results with unnested_topic_info_df, then results_extended with df_joined.

```{r}
results_extended <- results |> 
  left_join(unnested_topic_info_df |> distinct(Name, .keep_all = TRUE))
head(results_extended)

# results_extended |>
#   # distinct(Name, .keep_all = TRUE) |>
#   saveRDS("data/wm_topics_01_results_extended.rds")
# results_extended |>
#   # distinct(Name, .keep_all = TRUE) |>
#   write_csv("data/wm_topics_01_results_extended.csv") |>
#   writexl::write_xlsx("data/wm_topics_01_results_extended.xlsx")
```

```{r}
# Determine which column names are in df_joined but not in results_extended
columns_in_df_joined_not_in_results_extended <- setdiff(names(df_joined), names(results_extended))

# Determine which column names are in results_extended but not in df_joined
columns_in_results_extended_not_in_df_joined <- setdiff(names(results_extended), names(df_joined))

# Print the results
columns_in_df_joined_not_in_results_extended
columns_in_results_extended_not_in_df_joined
```


```{r}
df_all <- df_joined |> 
  left_join(results_extended)

df_all |> head()

# df_all |> 
#   saveRDS("data/wm_doc_topic_results_all.rds")
# df_all |> 
#   write_csv("data/wm_doc_topic_results_all.csv")
# df_all |> 
#   writexl::write_xlsx("data/wm_doc_topic_results_all.xlsx")
# df_all |> 
#   write_parquet("data/wm_doc_topic_results_all.parquet")
```


### Representative Docs

The representative documents of each topic are accessible like in the code chunk below, but the three documents are concatenated in one column (separated by a delimiter):

```{r}
unnested_doc_info_df1 |> 
  group_by(Topic) |> 
  filter(Topic == 0) |> 
  filter(Representative_document == TRUE) |>
  distinct(Representative_Docs) |>
  unnest(Representative_Docs)

```

Three representative documents per topic can be listed separately like this:

```{r}
document_info_df |>
  group_by(Topic) |> 
  # filter(Representative_document == TRUE) |> 
  filter(Topic == 0) |> 
  distinct(Representative_Docs) |> 
  unnest(Representative_Docs)

```


`BERTopic` also has a dedicated function for that task: `get_representative_docs()`. But the function returns only three documents (in our case: text chunks) for each topic. The representative documents of Topic 0 can obtained like this (`Python` is zero based, `R` is not, the outliers have the index 1, Topic 0 has index 2, etc.):

```{r}
topic_model$get_representative_docs()[2]
topic_model$get_representative_docs()$`0`

```

We can make a custom function `get_representative_docs_df()` to obtain representative documents using the dedicated BERTopic function `get_representative_docs()` out of the following program chunk: 

```{r}
get_representative_docs_df <- as.data.frame(topic_model$get_representative_docs()) |> 
  pivot_longer(everything(), 
               names_to = "Topic", values_to = "Representative_Docs") |> 
  mutate(Topic = str_replace(Topic, "X.1", "-1"),
         Topic = as.numeric(str_replace(Topic, "X", ""))) |> 
  arrange(Topic)

get_representative_docs_df |> filter(Topic == 0)
```


Custom function `get_representative_docs_df()`:

```{r}
get_representative_docs_df <- function(model=topic_model, topic_number=0){
  get_representative_docs_tbl <- topic_model$get_representative_docs() |>
    as_tibble() |> 
    pivot_longer(everything(), 
                 names_to = "Topic", values_to = "Representative_Docs") |> 
    mutate(Topic = str_replace(Topic, "X.1", "-1"),
           Topic = as.numeric(str_replace(Topic, "X", ""))) |> 
    arrange(Topic)
  
  if(topic_number < -1 | topic_number == "" ){
    return(get_representative_docs_tbl)
  }
  else{
    return(get_representative_docs_tbl |> 
             filter(Topic == as.integer(topic_number)))
  }
}

get_representative_docs_df(topic_model, "")
get_representative_docs_df(topic_model, 3)

```

The same (concatenation of the docs) is the case in our data frame df_all.

```{r}
df_all |> 
  filter(Topic == 3 & Representative_document == TRUE) |> 
  select(Topic, Representative_document, Representative_Docs, text_clean)
```

Below is a function that displays a *custom number of representative documents* of a certain topic.

```{r}
# Create a data frame
df_docs <- tibble(Topic = results_extended$Topic,
                  Document = results_extended$text_clean,
                  probs = results_extended$Probability)

# Define the function to get representative documents
get_representative_docs_df_custom <- function(df, topic_nr, n_docs) {
  # Filter the data frame to include only the specified topic
  df_filtered <- df %>%
    filter(Topic == topic_nr)
  
  # Randomly sample n_docs from the filtered data frame
  df_sampled <- df_filtered %>%
    sample_n(min(n_docs, nrow(df_filtered))) # Ensure not to sample more than available

  # Return the list of sampled documents
  return(df_sampled$Document)
}

# Example usage of the function
sampled_docs <- get_representative_docs_df_custom(df_docs, topic_nr = 3, n_docs = 5)
unique(sampled_docs)

```


```{r}
source("functions/get_representative_docs_custom.R")

# Example usage of the function

# Create a data frame from the topic model results
df_docs <- tibble(Topic = results$Topic,
                  Document = results$text_clean,
                  probs = results$Probability)
# use the custom function
sampled_docs <- get_representative_docs_df_custom(df_docs, topic_nr = 3, n_docs = 5)
unique(sampled_docs)

```


Below is another custom function for the most representative documents of a topic.

```{r}
# Create a data frame similar to df_docs
df_docs <- tibble(Topic = results$Topic,
                  Document = results$text_clean,
                  probs = results$Probability)

# Define the function to get the most representative documents
get_most_representative_docs <- function(df, topic_nr, n_docs = 5) {
  # Filter the data frame to include only the specified topic
  df_filtered <- df %>%
    filter(Topic == topic_nr)
  
  # Extract the probability scores for the specified topic
  topic_probs <- df_filtered$probs
  
  # Order the documents by their relevance to the topic, descending
  top_idx <- order(topic_probs, decreasing = TRUE)
  
  # Select the top n_docs most representative documents
  top_docs <- df_filtered$Document[top_idx[1:n_docs]]
  
  # Return the list of top representative documents
  return(top_docs)
}

# Example usage of the function
sampled_representative_docs <- get_most_representative_docs(df_docs, topic_nr = 3, 5)
unique(sampled_representative_docs)

```

More concise is the following custom function in tidyverse fashion.

```{r}
get_most_representative_docs_tidy <- function(tbl, topic_nr, n_docs=5){
  tbl |> 
    filter(Topic == topic_nr) |> 
    arrange(-Probability) |> 
    select(text_clean) |> 
    slice_head(n = n_docs) |> 
    pull(text_clean) |> 
    unique()
}

# Example usage
sampled_representative_docs <- get_most_representative_docs_tidy(results_extended, 3, 5)
unique(sampled_representative_docs)
```

Load the improved function from the functions folder;

```{r}
source("functions/get_most_representative_docs.R")

# Example usage

# Create a data frame similar to df_docs from topic model results
df_docs <- tibble(Topic = results$Topic,
                  Document = results$text_clean,
                  probs = results$Probability)

# Define the function to get the most representative documents
sampled_docs <- get_most_representative_docs(df_docs, 3, 5)
unique(sampled_docs)

```




### Document-topic probabilities

A *gamma matrix* in R `library(stm)` with document-topic probabilities:

```{r}
# document_names <- rownames(dataset)
document_names <- results$row_id
# document_names <- results$Doc_ID

# Convert the probabilities to a DataFrame
probs_df <- as.data.frame(probs)
colnames(probs_df) <- paste0("Topic_", 1:ncol(probs_df))
probs_df$Document <- document_names

# Convert the DataFrame to a tidy format
tidy_probs_df <- probs_df %>%
  pivot_longer(
    cols = starts_with("Topic_"),
    names_to = "topic",
    values_to = "probability"
  ) %>%
  mutate(topic = as.integer(gsub("Topic_", "", topic))) |> 
  # synchronize with Python
  mutate(topic = topic-1)

# View the tidy DataFrame
head(tidy_probs_df)

tidy_probs_df <- tidy_probs_df |> 
  mutate(row_id = Document)

gamma_enriched_df <- tidy_probs_df |> 
  full_join(results, join_by(row_id)) |> 
  rename(Topic_max = Topic, 
         Prob_max = Probability,
         Topic = topic,
         Probability = probability) |> 
  select(Document, row_id, Topic, Probability, Topic_max, Prob_max, everything())

# View the tidy DataFrame
head(gamma_enriched_df) |> 
  rmarkdown::paged_table()

```


```{r}
tidy_probs_df |> 
  filter(Document == "20") |> 
  arrange(-probability) |> 
  rmarkdown::paged_table()
```

```{r}
gamma_enriched_df |> 
  filter(Document == "20") |> 
  arrange(-Probability) |> 
  rmarkdown::paged_table()
```


```{r}
#| fig-width: 10
#| fig-height: 18
#| warning: false

# Probability = gamma
p01 <- gamma_enriched_df |> 
  # filter(Topic != 0) |> 
  mutate(Topic = factor(Topic)) |> 
  group_by(genre) |> 
  # normalize probabilities
  mutate(Probability = Probability / sum(Probability)) %>%
  group_by(genre, Topic) |> 
  # summarise(Probability = max(Probability)) |> 
  ggplot(aes(Probability, Topic, fill = Topic)) +
  # geom_boxplot(alpha = 0.7, show.legend = FALSE) +
  geom_col(alpha = 0.7) +
  facet_wrap(~ genre) +
  labs(x = expression(Probability),
       title = "Topic Probabilities by genre") +
  theme(legend.position = "none")
p01
# ggsave("pics/topics_unos_by_subject.png", dpi = 300, width = 10, height = 18)

# library(plotly)
# ggplotly(p01)
```


```{r}
#| fig-width: 15
#| fig-height: 12
#| warning: false

# Probability = gamma
p02 <- gamma_enriched_df |> 
  filter(Topic >= 0 & Topic <= 9) |>
  mutate(Topic = factor(Topic)) |> 
  group_by(genre, year) |> 
  # normalize probabilities
  mutate(Probability = Probability / sum(Probability)) %>%
  group_by(year, Topic) |> 
  # summarise(Probability = max(Probability)) |> 
  ggplot(aes(Probability, Topic, fill = Topic)) +
  # geom_boxplot(alpha = 0.7, show.legend = FALSE) +
  geom_col(alpha = 0.7) +
  facet_wrap(~ genre + year, scales = "free_y") +
  labs(x = expression(Probability),
       title = "Topic Probabilities by year") +
  theme(legend.position = "none")
p02
# ggsave("pics/topics_unos_by_subject.png", dpi = 300, width = 10, height = 10)

```


### Topic development

Visualize the temporal development of the topics in our documents with the `bertopic` function `visualize_topics_over_time()`.

```{r}
# topic_model.visualize_topics_over_time(topics_over_time, top_n_topics=20)
# timestamps <- as.integer(dataset$year)
# topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)

plotly <- import("plotly")
np <- import("numpy")

# Visualize topics
fig <- topic_model$visualize_topics_over_time(topics_over_time, top_n_topics=20L)

# # Render the plot using plotly
# plotly$offline$iplot(fig)

# Save the figure as an HTML file
plotly$offline$plot(fig, filename = "wm_topics_over_time_interactive.html", 
                    auto_open = TRUE)

library(htmltools)
library(htmlwidgets)
# Read the HTML file content as a single string
html_content <- read_file(
  "wm_topics_over_time_interactive.html")
# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```


```{r}
# # Dynamic topic model
# timestamps <- as.integer(dataset$year)
# topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)

# Load necessary libraries
# library(plotly) - # we use Python's plotly with BERTopic
library(reticulate)
library(readr)
library(htmltools)
library(htmlwidgets)

# Define the function to visualize topics over time
visualize_topics_over_time <- function(model = topic_model, 
                                       top_n_topics = 20, 
                                       filename = "topics_over_time") {
  
  # Import Python modules using reticulate
  plotly <- import("plotly")
  
  # Ensure the filename has no illegal characters for file naming
  filename <- gsub("[^[:alnum:]_]", "", filename)
  
  # Visualize topics over time using the BERTopic model
  fig <- topic_model$visualize_topics_over_time(topics_over_time, top_n_topics = as.integer(top_n_topics))
  
  # Save the figure as an HTML file
  plotly$offline$plot(fig, filename = paste0(filename, ".html"), auto_open = FALSE)
  
  # Read the HTML file content as a single string
  html_content <- read_file(paste0(filename, ".html"))
  
  # Display the saved HTML file content in the Quarto notebook
  browsable(HTML(html_content))
}

# Example usage
visualize_topics_over_time(model = topics_over_time, top_n_topics = 5, filename = "plot")

```

Load the improved function `visualize_topics_over_time()` from the `function` folder:

```{r}
source("functions/visualize_topics_over_time.R")
# assuming that the 'topics_over_time' model and 'timestamps' are ready
visualize_topics_over_time(model = topic_model, 
                           topics_over_time_model = topics_over_time, 
                           top_n_topics = 5, 
                           filename = "plot")
```


Visualize the temporal development of *selected* topics in our documents.

```{r}
# topic_model.visualize_topics_over_time(topics_over_time, topics=[9, 10, 72, 83, 87, 91])
# timestamps <- as.integer(dataset$year)
# topics_over_time  <- topic_model$topics_over_time(texts_cleaned, timestamps, nr_bins=20L, global_tuning=TRUE, evolution_tuning=TRUE)

plotly <- import("plotly")
np <- import("numpy")

# Visualize topics
fig <- topic_model$visualize_topics_over_time(topics_over_time, topics=c(0, 1, 2, 3, 4, 5))

# # Render the plot using plotly
# plotly$offline$iplot(fig)

# Save the figure as an HTML file
plotly$offline$plot(fig, filename = "wm_topics_over_time_select_interactive.html", 
                    auto_open = TRUE)

library(htmltools)
library(htmlwidgets)
# Read the HTML file content as a single string
html_content <- read_file(
  "wm_topics_over_time_select_interactive.html")
# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))


```


### Topic prob distribution

The `bertopic` function `visualize_distribution()` displays an interactive `plotly` diagram of the topic distribution in a selected document (in our case: text chunk).

```{r}
plotly <- import("plotly")

# Example of visualizing topic distribution for a specific document
abstract_id <- 208
topic_distr <- as.numeric(probs[abstract_id, ])
topic_distr_np <- np$array(topic_distr)

# Visualize the topic-document distribution for the specified document
fig <- topic_model$visualize_distribution(topic_distr_np, min_probability = 0.0)
# plotly$offline$iplot(fig)
plotly$offline$plot(fig, filename = "wm_topic_dist_interactive.html", auto_open = TRUE)

# Display the saved HTML file content in the Quarto notebook
html_content <- read_file("wm_topic_dist_interactive.html")
browsable(HTML(html_content))
```


```{r}
visualize_distribution <- function(model = topic_model, 
                                   text_id = 1, 
                                   probabilities = probs){
  plotly <- import("plotly")
  np <- import("numpy")
  
  # Example of visualizing topic distribution for a specific document
  text_id <- 208
  topic_distr <- as.numeric(probs[text_id, ])
  topic_distr_np <- np$array(topic_distr)
  
  # Visualize the topic-document distribution for the specified document
  fig <- topic_model$visualize_distribution(topic_distr_np, min_probability = 0.0)
  # plotly$offline$iplot(fig)
  plotly$offline$plot(fig, filename = "topic_dist_interactive.html", auto_open = FALSE)
  
  # Display the saved HTML file content in the Quarto notebook
  html_content <- read_file("wm_topic_dist_interactive.html")
  browsable(HTML(html_content))  
}

# Example usage
visualize_distribution(model = topic_model, text_id = 1, probabilities = probs)

```

Load the improved function `visualize_distribution()` from the functions folder:

```{r}
source("functions/visualize_distribution.R")
# Example usage
visualize_distribution(model = topic_model, text_id = 208, probabilities = probs)
```



Modify the visualization and saving steps to include the additional context.


### Visualize topics

We can utilize the `bertopic` function `visualize_topics()` to display the topic sizes and their relationships or closeness.

```{r}
# Visualize topics
fig <- topic_model$visualize_topics(custom_labels=FALSE)

# Extract the necessary data from the figure object directly
fig_dict <- fig$to_plotly_json()

# Extract the 'customdata' from the JSON structure
customdata <- fig_dict$data[[1]]$customdata

# Convert to dataframe
df <- as.data.frame(customdata, columns = c('Topic', 'Tokens', 'Size'))

unnested_df <- df |> 
  unnest(cols = V1, keep_empty = TRUE) |> 
  unnest(cols = V2, keep_empty = TRUE) |> 
  unnest(cols = V3, keep_empty = TRUE) |> 
  distinct()

colnames(unnested_df) <- c('Topic', 'Tokens', 'Size')

# Print the unnested topic information data frame
unnested_df |> 
  head(10)

# Convert types if necessary
unnested_df <- unnested_df %>%
  mutate(
    Topic = as.numeric(Topic),
    Tokens = as.character(Tokens),
    Size = as.integer(Size)
  )

pic <- unnested_df |> 
  slice_max(Size, n = 30) |> 
  ggplot(aes(x = reorder(Tokens, -Size), y = Size)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  theme_minimal() +
  labs(title = "Topic Sizes", x = "Tokens", y = "Size")
# ggsave("pics/spiegel_topics_keywords.png", plot = pic, dpi = 300, width = 10, height = 10)
plotly::ggplotly(pic)
```


An interactive *Intertopic Distance Map*, created with the `visualize_topics()` function and `plotly`.

```{r}
# Import the Python version of plotly for visualization
# BERTopic visualize method relies on that
plotly <- import("plotly")
np <- import("numpy")

# Visualize topics
fig <- topic_model$visualize_topics(custom_labels=FALSE)

# # Render the plot using plotly
# plotly$offline$iplot(fig)

# Save the figure as an HTML file
plotly$offline$plot(fig, filename = "wm_topics_size_interactive_2.html", 
                    auto_open = TRUE)

library(htmltools)
library(htmlwidgets)
# Read the HTML file content as a single string
html_content <- read_file(
  "wm_topics_size_interactive_2.html")
# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```

**Inspect Top Words of Overlapping Topics**: Identify and compare the top keywords for the topics that overlap on the map. Use `BERTopic`'s `get_topic` function.


```{r}
visualize_topics <- function(model=topic_model, filename="intertopic_distance_map"){
  # Import the Python version of plotly for visualization
  # BERTopic visualize method relies on that
  plotly <- import("plotly")

  # Visualize topics
  fig <- topic_model$visualize_topics(custom_labels=FALSE)
  
  # Save the figure as an HTML file
  plotly$offline$plot(fig, filename = paste0(filename, ".html"), 
                      auto_open = FALSE)
  
  library(htmltools)
  library(htmlwidgets)
  # Read the HTML file content as a single string
  html_content <- read_file(paste0(filename, ".html"))
  # Display the saved HTML file content in the Quarto notebook
  browsable(HTML(html_content))
  
}

# Example usage
visualize_topics(topic_model, "plot")

```

Load the improved `visualize_topics()` function from the `functions` folder:

```{r}
source("functions/visualize_topics.R")

# Example usage
visualize_topics(topic_model, "plot")

```


### Hierarchy of topics

With the help of `bertopic`'s functions `hierarchical_topics()`, `get_topic_tree()` and `visualize_hierarchy()`, we can obtain hierarchical representations of the extracted topics. The dendrograms clearly show which topics are similar and could be eventually merged. 

```{r}
# Hierarchical topics
# linkage_function = lambda x: sch.linkage(x, 'single', optimal_ordering=True)
hierarchical_topics  <- topic_model$hierarchical_topics(texts_cleaned) # , linkage_function = linkage_function)

tree <- topic_model$get_topic_tree(hierarchical_topics)
# print(tree)
```


```{r}
# topic_model.visualize_hierarchy(custom_labels=True)

# Generate the hierarchical clustering visualization
fig_hierarchy <- topic_model$visualize_hierarchy(custom_labels = FALSE)

# Save the figure as an HTML file in Python
plotly$offline$plot(fig_hierarchy, filename = "wm_hierarchy_interactive.html", auto_open = TRUE)

# Read the HTML file content as a single string
html_content <- read_file("wm_hierarchy_interactive.html")

# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```


```{r}
visualize_hierarchy <- function(model=topic_model, filename = "topic_hierarchy", auto_open = FALSE){
  plotly <- import("plotly")
  # Generate the hierarchical clustering visualization
  fig_hierarchy <- topic_model$visualize_hierarchy(custom_labels = FALSE)
  
  # Save the figure as an HTML file in Python
  plotly$offline$plot(fig_hierarchy, filename = paste0(filename, ".html"), auto_open = auto_open)
  
  # Read the HTML file content as a single string
  html_content <- read_file(paste0(filename, ".html"))
  
  # Display the saved HTML file content in the Quarto notebook
  browsable(HTML(html_content))  
}

# Example usage
visualize_hierarchy(model=topic_model, filename = "topic_hierarchy", auto_open=TRUE)

```

Load the improved `visualize_hierarchy()` function from the `functions` folder:

```{r}
source("functions/visualize_hierarchy.R")

# Example usage
hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned)
visualize_hierarchy(model = topic_model, 
                    hierarchical_topics = hierarchical_topics, # optional
                    filename = "topic_hierarchy", 
                    auto_open = FALSE)
```



```{r}
# hierarchical_topics = topic_model.hierarchical_topics(documents)
# topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)

hierarchical_topics = topic_model$hierarchical_topics(texts_cleaned)

# Generate the hierarchical clustering visualization
fig_hierarchical_topics <- topic_model$visualize_hierarchy(hierarchical_topics=hierarchical_topics, 
                                                           custom_labels = FALSE)

# Save the figure as an HTML file in Python
plotly$offline$plot(fig_hierarchical_topics, filename = "wm_hierarchical_topics_interactive.html", auto_open = TRUE)

# Read the HTML file content as a single string
html_content <- read_file("wm_hierarchical_topics_interactive.html")

# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))
```


### Groups & Classes

The following plot utilizes bertopic's functions `topics_per_class()` and `visualize_topics_per_class()` to display topic distribution depending on certain categories or groups in our dataset.

You can click on topics under the `legend` *Global Topic Representations* to the right to select or de-select topics to be included in the bar chart Hover over the bars to see the words the different parties were more likely to use for a given topic.

```{r}
# Python:
# classes = df["party"].tolist() # Party affiliation of authors
# topics_per_class = topic_model.topics_per_class(documents, classes=classes)
# topic_model.visualize_topics_per_class(topics_per_class, topics=list(range(0, 10)))

classes = as.list(dataset$genre) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)
# Visualize topics per class
fig_classes <- topic_model$visualize_topics_per_class(topics_per_class, topics = py$list(py$range(0L, 10L)))


# Save the figure as an HTML file in Python
plotly$offline$plot(fig_classes, filename = "wm_classes_topics_interactive.html", auto_open = TRUE)

# Read the HTML file content as a single string
html_content <- read_file("wm_classes_topics_interactive.html")

# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```


```{r}
classes <- as.list(dataset$artist)
# Calculate topics per class using the BERTopic model
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)
# Visualize topics per class
fig_classes <- topic_model$visualize_topics_per_class(topics_per_class, topics = py$list(py$range(0L, 10L)))


# Save the figure as an HTML file in Python
plotly$offline$plot(fig_classes, filename = "wm_classes_authors_topics_interactive.html", auto_open = TRUE)

# Read the HTML file content as a single string
html_content <- read_file("wm_classes_authors_topics_interactive.html")

# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```


```{r}
classes = as.list(dataset$genre) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)

# Visualize topics per class
visualize_topics_per_class <- function(model=topic_model,
                                       topics_per_class=topics_per_class, 
                                       start=0, end=10,
                                       filename="topics_per_class",
                                       auto_open=TRUE){
  fig_classes <- topic_model$visualize_topics_per_class(topics_per_class, topics = py$list(py$range(as.integer(start), as.integer(end))))
  
  # Save the figure as an HTML file in Python
  plotly$offline$plot(fig_classes, filename = paste0(filename, ".html"), auto_open = auto_open)
  
  # Read the HTML file content as a single string
  html_content <- read_file(paste0(filename, ".html"))
  
  # Display the saved HTML file content in the Quarto notebook
  browsable(HTML(html_content))
}

# Example usage
visualize_topics_per_class(model=topic_model, 
                           topics_per_class=topics_per_class, 
                           start=4, end=10, 
                           filename="plot",
                           auto_open = TRUE)

```

Load the improvement visualize_topics_per_class() function from the functions folder.

```{r}
source("functions/visualize_topics_per_class.R")

classes = as.list(dataset$genre) # text types
topics_per_class = topic_model$topics_per_class(texts_cleaned, classes=classes)

visualize_topics_per_class(model=topic_model, 
                           topics_per_class=topics_per_class, 
                           start=4, end=10, 
                           filename="plot",
                           auto_open = TRUE)
```



### Visualize documents

After some dimension reduction we can utilize the `visualize_documents()` function of `bertopic` to display the topic clusters. 

#### 2D interactive

```{r}
# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)

# Generate the document visualization
fig_documents <- topic_model$visualize_documents(texts_cleaned, reduced_embeddings = reduced_embeddings, custom_labels = TRUE, hide_annotations=TRUE)

# Save the figure as an HTML file in Python
plotly$offline$plot(fig_documents, filename = "wm_documents_interactive.html", auto_open = TRUE)

# Read the HTML file content as a single string
html_content <- read_file("wm_documents_interactive.html")

# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))
```

The resulting visualization is not as clear if the dimensions are not additionally reduced. Below it is disabled. 

```{r}
#| eval: false

# # Reduce dimensionality of embeddings using UMAP
# reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)

# Generate the document visualization
fig_documents <- topic_model$visualize_documents(texts_cleaned, reduced_embeddings = embeddings, custom_labels = TRUE, hide_annotations=TRUE)

# Save the figure as an HTML file in Python
plotly$offline$plot(fig_documents, filename = "wm_documents_full_interactive.html", auto_open = TRUE)

# Read the HTML file content as a single string
html_content <- read_file("wm_documents_full_interactive.html")

# Custom JavaScript to modify the click behavior
custom_js <- '
<script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
    var plot = document.getElementById("plotly-graph-div");

    plot.on("plotly_click", function(data) {
        var clickedTopic = data.points[0].curveNumber;
        
        var update = {
            visible: Array(plot.data.length).fill(false)
        };
        
        update.visible[clickedTopic] = true;
        
        Plotly.restyle(plot, update);
    });
});
</script>
'

# Insert the custom JavaScript before the closing </body> tag
html_content <- sub("</body>", paste(custom_js, "</body>"), html_content)

# Write the modified HTML content back to the file
write_file(html_content, "wm_documents_full_interactive.html")

# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))
```


```{r}
# Reduce dimensionality of embeddings using UMAP
reduced_embeddings <- umap$UMAP(n_neighbors = 10L, n_components = 2L, min_dist = 0.0, metric = 'cosine')$fit_transform(embeddings)

# Generate the document visualization
visualize_documents <- function(model=topic_model,
                                texts=texts_cleaned,
                                reduced_embeddings=reduced_embeddings,
                                custom_labels=FALSE,
                                hide_annotation=TRUE,
                                filename="visualize_documents",
                                auto_open=FALSE){
  library(reticulate)
  plotly <- import("plotly")
  
  fig_documents <- topic_model$visualize_documents(texts_cleaned, reduced_embeddings = reduced_embeddings, custom_labels = FALSE, hide_annotations=TRUE)
  
  # Save the figure as an HTML file in Python
  plotly$offline$plot(fig_documents, filename = paste0(filename, ".html"), auto_open = auto_open)
  
  # Read the HTML file content as a single string
  html_content <- read_file(paste0(filename, ".html"))
  
  # Display the saved HTML file content in the Quarto notebook
  browsable(HTML(html_content))  
}

# Example usage
visualize_documents(model=topic_model,
                                texts=texts_cleaned,
                                reduced_embeddings=reduced_embeddings,
                                custom_labels=FALSE,
                                hide_annotation=TRUE,
                                filename="visualize_documents",
                                auto_open=FALSE)

```


```{r}
source("functions/visualize_documents.R")

visualize_documents(model=topic_model,
                                texts=texts_cleaned,
                                reduced_embeddings=reduced_embeddings,
                                custom_labels=FALSE,
                                hide_annotation=TRUE,
                                filename="visualize_documents",
                                auto_open=TRUE)
```


#### 3D interactive plot.

Our topic model, simple tooltip (only Topic number). 

```{r}
# Import necessary Python modules using reticulate
plotly <- import("plotly")
np <- import("numpy")
umap <- import("umap")
bertopic <- import("bertopic")
HDBSCAN <- hdbscan$HDBSCAN

# Extract the document embeddings from the trained topic model object
# embeddings <- topic_model$embedding_model_$transform(texts_cleaned)

# Use UMAP to reduce embeddings to 3D
fit <- UMAP(n_neighbors=15L, n_components=5L, min_dist=0.0, metric='cosine', random_state=42L)
u <- fit$fit_transform(embeddings) # original high-dimensional embeddings

clusterer <- HDBSCAN(min_cluster_size=23L, min_samples = 17L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)
clusterer$fit(u)

# Get the topic assignments
topics <- topic_model$get_document_info(texts_cleaned)$Topic

# Creating a 3D scatter plot using Plotly
fig <- plotly$graph_objs$Figure()

fig$add_trace(
  plotly$graph_objs$Scatter3d(
    x = u[,1],  # x-coordinates from UMAP
    y = u[,2],  # y-coordinates from UMAP
    z = u[,3],  # z-coordinates from UMAP
    mode = 'markers',
    marker = list(
      size = 3,
      color = topics,  # Color points by topic assignment
      colorscale = 'Viridis',
      opacity = 0.8
    ),
    text = paste("Topic:", topics),  # Tooltip text
    hoverinfo = 'text'
  )
)

# Save the figure as an HTML file
plotly$offline$plot(fig, filename = "topic_model_3d_visualization.html", auto_open = TRUE)

# Use R to display the saved HTML content
library(htmltools)
library(htmlwidgets)
# Read the HTML file content as a single string
html_content <- read_file("topic_model_3d_visualization.html")
# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```


Addition of School and Doc_ID to tooltips of the 3D interactive plot.
Toggle Topics: Double-clicking turns on / off Topic. Topics sorted. This plot is enaabled since it provides additional information.

```{r}
# Import necessary Python modules using reticulate
plotly <- import("plotly")
np <- import("numpy")
umap <- import("umap")
bertopic <- import("bertopic")
hdbscan <- import("hdbscan")
HDBSCAN <- hdbscan$HDBSCAN

# Assuming you have the trained topic_model object
# Extract the document embeddings from the topic model
# embeddings <- topic_model$embedding_model_$transform(texts_cleaned)

# Use UMAP to reduce embeddings to 3D
fit <- UMAP(n_neighbors=15L, n_components=3L, min_dist=0.0, metric='cosine', random_state=42L)
u <- fit$fit_transform(embeddings) # original high-dimensional embeddings

# Use HDBSCAN clustering
clusterer <- HDBSCAN(min_cluster_size=23L, min_samples = 17L, metric='euclidean', cluster_selection_method='eom', gen_min_span_tree=TRUE, prediction_data=TRUE)
clusterer$fit(u)

# Get the topic assignments
doc_info <- topic_model$get_document_info(texts_cleaned)
topics <- doc_info$Topic

# Use results_extended directly, as it contains all necessary information
results_extended <- results_extended  # Assuming this is defined as shown in your creation script

# Initialize an empty vector to store tooltips
tooltips <- vector("character", length(topics))

# Populate the tooltips vector using information from results_extended
for (i in seq_along(topics)) {
  row_info <- results_extended[i, ]  # Directly access the i-th row
  
  if (nrow(row_info) > 0) {
    tooltips[i] <- paste(
      "Topic:", row_info$Topic, "<br>",
      "LLM:", row_info$OpenAI, "<br>",
      "Name:", row_info$Name, "<br>",
      "Representation:", row_info$Representation, "<br>",
      "KeyBERT:", row_info$KeyBERT, "<br>",
      "Count:", row_info$Count, "<br>",
      "Genre:", row_info$genre, "<br>",
      "Artist:", row_info$artist, "<br>",
      "Title:", row_info$title, "<br>",
      "Year:", row_info$year, "<br>",
      "Text:", str_sub(row_info$text_clean, 1, 140), "<br>"
    )
  } else {
    # Handle cases where no information is available
    tooltips[i] <- paste("Topic:", topics[i], "<br>", "Information not available")
  }
}

# Creating a 3D scatter plot using Plotly
fig <- plotly$graph_objs$Figure()

# Sort unique topics
unique_topics <- sort(unique(topics))

# Loop through each sorted unique topic to create a separate trace for each
for (topic_id in unique_topics) {
  topic_indices <- which(topics == topic_id)
  
  fig$add_trace(
    plotly$graph_objs$Scatter3d(
      x = u[topic_indices, 1],  # x-coordinates for this topic
      y = u[topic_indices, 2],  # y-coordinates for this topic
      z = u[topic_indices, 3],  # z-coordinates for this topic
      mode = 'markers',
      marker = list(
        size = 3,
        opacity = 0.8
      ),
      name = paste("Topic", topic_id),  # Name for legend
      legendgroup = as.character(topic_id),  # Group by topic for toggling
      showlegend = TRUE,  # Show legend entry
      text = tooltips[topic_indices],  # Tooltip text
      hoverinfo = 'text'
    )
  )
}

# Save the figure as an HTML file
plotly$offline$plot(fig, filename = "topic_model_3d_visualization.html", auto_open = TRUE)

# Use R to display the saved HTML content
library(htmltools)
library(htmlwidgets)
# Read the HTML file content as a single string
html_content <- read_file("topic_model_3d_visualization.html")
# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```


## Save and Load the Model

Ensure to save and reload the model while maintaining the additional context.

```{r}
#| eval: false

# Save the model
topic_model$save("topic_model_bertopic_wm")

# Load the model
loaded_model <- BERTopic$load("topic_model_bertopic_wm")

# Verify loaded model
loaded_fit_transform <- loaded_model$fit_transform(texts_cleaned, embeddings)
loaded_topics <- loaded_fit_transform[[1]]
loaded_probs <- loaded_fit_transform[[2]]

# Combine results with additional columns
loaded_results <- data.frame(
  School = dataset$School,
  Doc_ID = dataset$Doc_ID,
  Chapter_Index = dataset$Chapter_Index,
  Book_Index = dataset$Book_Index,
  Sentence_ID = dataset$Sentence_ID,
  text_clean = texts_cleaned,
  Text = dataset$Text,
  Text_Short = dataset$Text_Short,
  Text_Length = dataset$text_length,
  Environmental_Phrases.x = dataset$Environmental_Phrases.x,
  Environmental_Phrases.y = dataset$Environmental_Phrases.y,
  Environmental_Phrases = dataset$Environmental_Phrases,
  Query = dataset$Query,
  Score = dataset$Score,
  Topic = loaded_topics,
  Probability = apply(loaded_probs, 1, max)
)

head(loaded_results)
```

This script ensures that the additional columns (`School`, `Doc_ID`, `Chapter_Index`, `Book_Index`, `Sentence_ID`) are included in the resulting dataframes and visualizations, providing full context for each topic identified. Adjust the script as necessary to suit your specific needs and data structure.


## Merge topics

Before merging, inspect the dendrogram to find out which topics are similar.

In the program chunks below, merging topics is disabled. To enable merging, set `eval` to *true*. 

```{r}
#| eval: false

topics_to_merge <- list(54L, 67L, 20L)
topic_model$merge_topics(texts_cleaned, topics_to_merge)

```

Choose a `list` of `lists` format when more than one group is intended for merging.

```{r}
#| eval: false

topics_to_merge <- list(list(54L, 67L, 20L),
                        list(101L,53L, 31L, 95L))
topic_model$merge_topics(texts_cleaned, topics_to_merge)

```


Inspect the changes in the dendrogram after merging topics.

```{r}
#| eval: false

# Generate the hierarchical clustering visualization
fig_hierarchy <- topic_model$visualize_hierarchy(custom_labels = FALSE)

# Save the figure as an HTML file in Python
plotly$offline$plot(fig_hierarchy, filename = "wm_hierarchy_merged_topics_interactive.html", auto_open = TRUE)

# Read the HTML file content as a single string
html_content <- read_file("wm_hierarchy_merged_topics_interactive.html")

# Display the saved HTML file content in the Quarto notebook
browsable(HTML(html_content))

```


## Outlier reduction

### Default

```{r}
#| eval: true

# Reduce outliers
new_topics = topic_model$reduce_outliers(texts_cleaned, topics)
```


### Probabilites

```{r}
#| eval: false

# Reduce outliers using the `probabilities` strategy
new_topics = topic_model$reduce_outliers(texts_cleaned, topics, probabilities=probs, 
                             threshold=0.05, strategy="probabilities")
```


### c-TF-IDF

```{r}
#| eval: false

# Use the "c-TF-IDF" strategy with a threshold
new_topics = topic_model$reduce_outliers(texts_cleaned, topics , strategy="c-tf-idf", threshold=0.1)

```


### Distributions

```{r}
#| eval: false

# Reduce all outliers that are left with the "distributions" strategy
new_topics = topic_model$reduce_outliers(texts_cleaned, topics, strategy="distributions")

```


### Embeddings

```{r}
#| eval: false

# Reduce outliers with pre-calculate embeddings instead
new_topics = topic_model$reduce_outliers(texts_cleaned, topics, strategy="embeddings", embeddings=embeddings)

```


### Chain Strategies

```{r}
#| eval: false

# Reduce outliers with pre-calculate embeddings instead
new_topics = topic_model$reduce_outliers(texts_cleaned, topics, strategy="embeddings", embeddings=embeddings)

# Use the "c-TF-IDF" strategy with a threshold
new_topics = topic_model$reduce_outliers(texts_cleaned, new_topics , strategy="c-tf-idf", threshold=0.1)

# Reduce outliers using the `probabilities` strategy
new_topics = topic_model$reduce_outliers(texts_cleaned, new_topics, probabilities=probs, strategy="probabilities")

# Reduce all outliers that are left with the "distributions" strategy
new_topics = topic_model$reduce_outliers(texts_cleaned, new_topics, strategy="distributions")

```

## Update topics

```{r}
topic_model$update_topics(texts_cleaned, topics=new_topics)
```

